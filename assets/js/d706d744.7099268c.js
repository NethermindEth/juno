"use strict";(self.webpackChunkjuno_docs=self.webpackChunkjuno_docs||[]).push([[1865],{7649:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var s=t(4848),a=t(8453);const i={title:"Performance Tuning"},o=void 0,r={id:"tuning",title:"Performance Tuning",description:"It is important for full nodes to scale accordingly to the hardware where they are being executed. To unlock this, the following are a list of configurations users can update based on their hardware specs to maximize the performance of their Juno node.",source:"@site/docs/tuning.md",sourceDirName:".",slug:"/tuning",permalink:"/next/tuning",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"Performance Tuning"},sidebar:"main",previous:{title:"Updating Juno",permalink:"/next/updating"},next:{title:"Deploy on GCP",permalink:"/next/running-on-gcp"}},c={},l=[{value:"Database Compression",id:"database-compression",level:2},{value:"Database Memory Table Size",id:"database-memory-table-size",level:2},{value:"Database Memory Table Count",id:"database-memory-table-count",level:2},{value:"Database Compaction Concurrency",id:"database-compaction-concurrency",level:2},{value:"Database Cache Size",id:"database-cache-size",level:2}];function d(e){const n={admonition:"admonition",code:"code",h2:"h2",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"It is important for full nodes to scale accordingly to the hardware where they are being executed. To unlock this, the following are a list of configurations users can update based on their hardware specs to maximize the performance of their Juno node."}),"\n",(0,s.jsxs)(n.p,{children:["The default values of each of these options are set to maximize performance with a machine that matches Juno's minimum requirements \u2013 described in the ",(0,s.jsx)(n.strong,{children:"Hardware Requirement"})," section."]}),"\n",(0,s.jsx)(n.h2,{id:"database-compression",children:"Database Compression"}),"\n",(0,s.jsxs)(n.p,{children:["Set by the ",(0,s.jsx)(n.code,{children:"--db-compression"})," flag, it applies a compression algorithm over the database ",(0,s.jsx)(n.strong,{children:"every time"})," Juno writes to it."]}),"\n",(0,s.jsx)(n.p,{children:"Available options:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"snappy"}),": Fast compression with a low compression ratio"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"zstd"}),": Slower but reduces storage quite a lot"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"minlz"}),": Alternative compression option"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Depending on the compression algorithm used it becomes a trade-off between ",(0,s.jsx)(n.strong,{children:"disk space"})," and ",(0,s.jsx)(n.strong,{children:"CPU"})," usage every time there is a disk operation."]}),"\n",(0,s.jsxs)(n.p,{children:["We recommend ",(0,s.jsx)(n.code,{children:"zstd"})," because it is fast enough that it doesn't delays any process significantly while providing huge database size reduction."]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"Note that once the compression is changed the new database is not compressed immediately, but gradually through the node usage by writing new information."})}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsxs)(n.p,{children:["There is a secret ",(0,s.jsx)(n.code,{children:"zstd1"})," option that provides far greater performance than ",(0,s.jsx)(n.code,{children:"zstd"})," but we are still testing it out and it might become the default later."]})}),"\n",(0,s.jsx)(n.h2,{id:"database-memory-table-size",children:"Database Memory Table Size"}),"\n",(0,s.jsxs)(n.p,{children:["Set by the ",(0,s.jsx)(n.code,{children:"--db-memtable-size"})," flag (default: 256 MB), this controls the amount of memory allocated for the database memtable. The memtable is an in-memory buffer where writes are stored before being flushed to disk."]}),"\n",(0,s.jsxs)(n.p,{children:["A sensible default is ",(0,s.jsx)(n.strong,{children:"256 MB"})," for nodes that satisfy the minimum requirements. Increasing this value reduces the frequency of disk flushes, which can improve write throughput during sync."]}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["Setting this value too high can cause ",(0,s.jsx)(n.strong,{children:"uneven write performance"}),". Larger memtables mean flushes happen less frequently but involve more data at once, leading to bursty I/O patterns. If writes accumulate faster than the database can flush, writes will stall entirely until flushing catches up. A moderate value like 256 MB balances flush frequency with I/O smoothness."]})}),"\n",(0,s.jsx)(n.h2,{id:"database-memory-table-count",children:"Database Memory Table Count"}),"\n",(0,s.jsxs)(n.p,{children:["Set by the ",(0,s.jsx)(n.code,{children:"--db-memtable-count"})," flag (default: 2), this controls the number of memtables the database can hold in memory before stalling writes. Memtables are in-memory buffers where writes accumulate before being flushed to disk."]}),"\n",(0,s.jsx)(n.p,{children:"With the default of 2, Pebble can accept writes into one memtable while flushing a previous one to disk. Increasing this value allows more memtables to be queued, which can absorb write bursts and prevent stalls when flush speed falls behind write speed."}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsxs)(n.p,{children:["Each additional memtable consumes up to ",(0,s.jsx)(n.code,{children:"--db-memtable-size"})," MB of memory. For example, with the default memtable size of 256 MB and a count of 4, up to 1 GB of memory could be used for memtables alone."]})}),"\n",(0,s.jsx)(n.h2,{id:"database-compaction-concurrency",children:"Database Compaction Concurrency"}),"\n",(0,s.jsxs)(n.p,{children:["Set by the ",(0,s.jsx)(n.code,{children:"--db-compaction-concurrency"})," flag, this controls how many concurrent compaction workers the database uses. Compaction is the background process that merges and optimises data on disk."]}),"\n",(0,s.jsx)(n.p,{children:"Format options:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"N"}),": Sets the range from 1 to N workers (e.g., ",(0,s.jsx)(n.code,{children:"--db-compaction-concurrency=4"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"M,N"}),": Sets the range from M to N workers (e.g., ",(0,s.jsx)(n.code,{children:"--db-compaction-concurrency=2,8"}),")"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The default is ",(0,s.jsx)(n.code,{children:"1,GOMAXPROCS/2"})," (half of available CPU cores). Increasing the upper bound on systems with many cores can speed up compaction as well as increase CPU resources usage while syncing parts of the network where there was a lot of usage."]}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"Note that this effectively improve syncing speed while behind the tip of the chain but after reaching the latest block resource usage will gradually reduce back to minimums."})}),"\n",(0,s.jsx)(n.h2,{id:"database-cache-size",children:"Database Cache Size"}),"\n",(0,s.jsxs)(n.p,{children:["Set by the ",(0,s.jsx)(n.code,{children:"--db-cache-size"})," flag (default: 1024 MB), this determines the amount of memory allocated for caching frequently accessed data from the database."]}),"\n",(0,s.jsx)(n.p,{children:"A larger cache reduces disk reads and improves query performance. On systems with ample memory, increasing this value (e.g., 2048 or 4096 MB) can significantly improve RPC response times and overall node performance."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const a={},i=s.createContext(a);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);